import os
import time
import sys
from contextlib import contextmanager
import numpy as np
from funasr import AutoModel
import noisereduce as nr
from meeting_mind.app.core.config import settings
from meeting_mind.app.core.logger import logger


class ASREngine:
    _instance = None

    # Constants
    SAMPLE_RATE = 16000
    BYTES_PER_SAMPLE = 2  # int16
    SAMPLES_PER_MS = int(SAMPLE_RATE / 1000)  # 16
    BYTES_PER_MS = int(SAMPLE_RATE * BYTES_PER_SAMPLE / 1000)  # 32
    PCM_NORM_FACTOR = 32768.0

    # Thresholds
    MIN_SPEAKER_AUDIO_LEN_SEC = 0.5
    MIN_SEGMENT_LEN_SEC = 0.3  # æé«˜æœ€å°æ®µé•¿åº¦ï¼Œè¿‡æ»¤å™ªå£°ç‰‡æ®µ
    MAX_SEGMENT_DURATION_MS = 8000  # é™ä½å¼ºåˆ¶æ–­å¥æ—¶é•¿ï¼Œä½¿å¥å­æ›´è‡ªç„¶
    SPEAKER_SIMILARITY_THRESHOLD = 0.35  # æé«˜é˜ˆå€¼ï¼Œå‡å°‘è¯´è¯äººè¯¯åˆ¤
    NOISE_REDUCTION_PROP = 0.75  # ç•¥å¾®é™ä½é™å™ªå¼ºåº¦ï¼Œä¿ç•™æ›´å¤šè¯­éŸ³ç»†èŠ‚
    GENDER_FREQ_THRESHOLD = 165

    # VAD
    VAD_CHUNK_SIZE = 300  # å¢å¤§chunk sizeï¼Œå‡å°‘ç¢ç‰‡åŒ–åˆ†æ®µ

    # é™éŸ³æ£€æµ‹
    SILENCE_ENERGY_THRESHOLD = 150.0  # é™éŸ³èƒ½é‡é˜ˆå€¼
    SILENCE_DURATION_MS = 400  # é™éŸ³æ—¶é•¿é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤æ—¶é•¿è®¤ä¸ºæ˜¯åœé¡¿

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(ASREngine, cls).__new__(cls)
            cls._instance.asr_model = None
            cls._instance.vad_model = None
            cls._instance.punc_model = None
            cls._instance.cache = {}
            cls._instance.speaker_registry = {}

            # Queue for async processing
            cls._instance.queue = None
            cls._instance.worker_task = None
            cls._instance.callbacks = {}

        return cls._instance

    @contextmanager
    def _suppress_stdout(self):
        """ä¸´æ—¶å±è”½ stdout ä»¥éšè—ç¬¬ä¸‰æ–¹åº“çš„ print è¾“å‡º"""
        with open(os.devnull, "w") as devnull:
            old_stdout = sys.stdout
            sys.stdout = devnull
            try:
                yield
            finally:
                sys.stdout = old_stdout

    def load_models(self):
        """åˆ†åˆ«åŠ è½½ FunASR æ¨¡å‹åˆ°å†…å­˜ä¸­ã€‚"""
        if self.asr_model is not None:
            logger.info("æ¨¡å‹å·²åŠ è½½ã€‚")
            return

        logger.info("æ­£åœ¨åŠ è½½ FunASR æ¨¡å‹...")
        try:
            with self._suppress_stdout():
                # åŠ è½½ ASR æ¨¡å‹
                self.asr_model = AutoModel(
                    model=settings.ASR_MODEL_PATH,
                    disable_update=True,
                    device=settings.ASR_DEVICE,
                )

                # åŠ è½½ VAD æ¨¡å‹
                self.vad_model = AutoModel(
                    model=settings.VAD_MODEL_PATH,
                    disable_update=True,
                    device=settings.ASR_DEVICE,
                )

                # åŠ è½½æ ‡ç‚¹æ¨¡å‹
                self.punc_model = AutoModel(
                    model=settings.PUNC_MODEL_PATH,
                    disable_update=True,
                    device=settings.ASR_DEVICE,
                )

                # åŠ è½½è¯´è¯äººæ¨¡å‹
                self.speaker_model = AutoModel(
                    model=settings.SPEAKER_MODEL_PATH,
                    disable_update=True,
                    device=settings.ASR_DEVICE,
                )

            if self.asr_model:
                logger.info("  âœ“ ASR æ¨¡å‹åŠ è½½æˆåŠŸ")
            if self.vad_model:
                logger.info("  âœ“ VAD æ¨¡å‹åŠ è½½æˆåŠŸ")
            if self.punc_model:
                logger.info("  âœ“ æ ‡ç‚¹æ¨¡å‹åŠ è½½æˆåŠŸ")
            if self.speaker_model:
                logger.info("  âœ“ è¯´è¯äººæ¨¡å‹åŠ è½½æˆåŠŸ")
            logger.info("ğŸ‰ è¯­éŸ³è¯†åˆ«æ‰€éœ€æ¨¡å‹åŠ è½½å®Œæˆ!")
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ¨¡å‹å‡ºé”™: {e}")
            raise e

    def _cosine_similarity(self, v1, v2):
        """è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
        v1 = np.squeeze(v1)
        v2 = np.squeeze(v2)

        norm1 = np.linalg.norm(v1)
        norm2 = np.linalg.norm(v2)
        if norm1 == 0 or norm2 == 0:
            return 0.0
        return np.dot(v1, v2) / (norm1 * norm2)

    def detect_gender(self, audio_chunk: bytes):
        """
        åŸºäºéŸ³é«˜æ£€æµ‹æ€§åˆ« - ä½¿ç”¨å¤šæŒ‡æ ‡ç»¼åˆåˆ¤æ–­

        ä¼˜åŒ–æ–¹æ¡ˆï¼š
        1. ä½¿ç”¨ä¸­ä½æ•°è€Œéå¹³å‡å€¼ï¼Œæ›´æŠ—å™ªå£°
        2. å¢åŠ æœ‰æ•ˆè¯­éŸ³å¸§æ¯”ä¾‹æ£€æŸ¥
        3. ä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼èŒƒå›´ï¼Œå¯¹è¾¹ç•Œæƒ…å†µè¿”å›æœªçŸ¥
        """
        try:
            import librosa

            # æœ€å°éŸ³é¢‘é•¿åº¦æ£€æŸ¥ (è‡³å°‘éœ€è¦0.5ç§’)
            if len(audio_chunk) < self.SAMPLE_RATE * 0.5 * self.BYTES_PER_SAMPLE:
                return "æœªçŸ¥"

            # è½¬æ¢ä¸º float32
            audio_np = (
                np.frombuffer(audio_chunk, dtype=np.int16).astype(np.float32) / 32768.0
            )

            # ä½¿ç”¨ librosa.pyin è¿›è¡ŒéŸ³é«˜è·Ÿè¸ª
            f0, voiced_flag, voiced_probs = librosa.pyin(
                audio_np,
                fmin=librosa.note_to_hz("C2"),  # ~65 Hz
                fmax=librosa.note_to_hz("C6"),  # ~1047 Hz
                sr=self.SAMPLE_RATE,
                frame_length=2048,
            )

            # è¿‡æ»¤ NaN å¹¶è·å–æœ‰æ•ˆå¸§
            valid_f0 = f0[~np.isnan(f0)]

            # æ£€æŸ¥æœ‰æ•ˆè¯­éŸ³å¸§æ¯”ä¾‹ï¼ˆè‡³å°‘30%çš„å¸§æœ‰æœ‰æ•ˆéŸ³é«˜ï¼‰
            voiced_ratio = len(valid_f0) / len(f0) if len(f0) > 0 else 0

            if len(valid_f0) < 5 or voiced_ratio < 0.3:
                logger.debug(
                    f"æœ‰æ•ˆè¯­éŸ³å¸§ä¸è¶³: {len(valid_f0)}, æ¯”ä¾‹: {voiced_ratio:.2%}"
                )
                return "æœªçŸ¥"

            # ä½¿ç”¨ä¸­ä½æ•°æ›´ç¨³å¥
            median_pitch = np.median(valid_f0)
            # åŒæ—¶è®¡ç®—å››åˆ†ä½è·æ£€æŸ¥å˜å¼‚æ€§
            q1, q3 = np.percentile(valid_f0, [25, 75])
            iqr = q3 - q1

            logger.debug(
                f"éŸ³é«˜ç»Ÿè®¡: ä¸­ä½æ•°={median_pitch:.1f}Hz, Q1={q1:.1f}, Q3={q3:.1f}, IQR={iqr:.1f}"
            )

            # ä½¿ç”¨é˜ˆå€¼èŒƒå›´ï¼Œè¾¹ç•Œæƒ…å†µè¿”å›æœªçŸ¥
            # ç”·æ€§å…¸å‹èŒƒå›´: 85-180 Hz
            # å¥³æ€§å…¸å‹èŒƒå›´: 165-255 Hz
            # é‡å åŒºåŸŸ 165-180 Hz è¾ƒéš¾åˆ¤æ–­

            if median_pitch < 150:
                return "ç”·"
            elif median_pitch > 190:
                return "å¥³"
            else:
                # è¾¹ç•ŒåŒºåŸŸï¼Œæ ¹æ®åˆ†å¸ƒå½¢æ€è¾…åŠ©åˆ¤æ–­
                # å¦‚æœå˜å¼‚èŒƒå›´åä½ï¼Œå¯èƒ½æ˜¯ç”·æ€§
                if q3 < 170:
                    return "ç”·"
                elif q1 > 160:
                    return "å¥³"
                else:
                    return "æœªçŸ¥"

        except Exception as e:
            logger.error(f"æ€§åˆ«æ£€æµ‹å¤±è´¥: {e}")
            return "æœªçŸ¥"

    def recognize_speaker(self, audio_segment: bytes, previous_speaker: str = None):
        """
        æå–è¯´è¯äººå£°çº¹ç‰¹å¾å¹¶è¯†åˆ«è¯´è¯äººã€‚

        Args:
            audio_segment: éŸ³é¢‘å­—èŠ‚æ•°æ®
            previous_speaker: ä¸Šä¸€ä¸ªè¯†åˆ«çš„è¯´è¯äººIDï¼Œç”¨äºè¿ç»­æ€§åˆ¤æ–­
        """
        if not self.speaker_model:
            logger.warning("è¯´è¯äººæ¨¡å‹æœªåŠ è½½")
            return "æœªçŸ¥"

        if (
            len(audio_segment)
            < self.SAMPLE_RATE * self.MIN_SPEAKER_AUDIO_LEN_SEC * self.BYTES_PER_SAMPLE
        ):
            logger.debug(f"éŸ³é¢‘ç‰‡æ®µå¤ªçŸ­ï¼Œæ— æ³•è¿›è¡Œè¯´è¯äººè¯†åˆ«: {len(audio_segment)} å­—èŠ‚")
            # å¦‚æœéŸ³é¢‘å¤ªçŸ­ï¼Œä¼˜å…ˆè¿”å›ä¸Šä¸€ä¸ªè¯´è¯äººä»¥ä¿æŒè¿ç»­æ€§
            if previous_speaker and previous_speaker in self.speaker_registry:
                return previous_speaker
            return "æœªçŸ¥"

        try:
            # å°†å­—èŠ‚è½¬æ¢ä¸º numpy æ•°ç»„
            audio_np = (
                np.frombuffer(audio_segment, dtype=np.int16).astype(np.float32)
                / self.PCM_NORM_FACTOR
            )

            res = self.speaker_model.generate(input=audio_np, disable_pbar=True)

            if isinstance(res, list) and len(res) > 0:
                embedding = res[0].get("spk_embedding")
                if embedding is not None:
                    # å¦‚æœæ˜¯ Tensor (GPU/CPU)ï¼Œè½¬æ¢ä¸º numpy
                    if hasattr(embedding, "cpu"):
                        embedding = embedding.detach().cpu().numpy()
                    elif hasattr(embedding, "numpy"):
                        embedding = embedding.numpy()

                    embedding = np.squeeze(embedding)
                    THRESHOLD = self.SPEAKER_SIMILARITY_THRESHOLD

                    best_score = -1.0
                    second_best_score = -1.0
                    best_speaker = None

                    # ä¸æ³¨å†Œçš„è¯´è¯äººè¿›è¡Œæ¯”è¾ƒ
                    for spk_id, data in self.speaker_registry.items():
                        spk_emb = data["embedding"]
                        score = self._cosine_similarity(embedding, spk_emb)
                        if score > best_score:
                            second_best_score = best_score
                            best_score = score
                            best_speaker = spk_id
                        elif score > second_best_score:
                            second_best_score = score

                    # è®¡ç®—ç½®ä¿¡åº¦å·®è· - å¦‚æœæœ€ä½³å’Œæ¬¡ä½³å·®è·å°ï¼Œè¯´æ˜ä¸ç¡®å®š
                    confidence_gap = (
                        best_score - second_best_score
                        if second_best_score > 0
                        else best_score
                    )

                    logger.debug(
                        f"è¯´è¯äººåŒ¹é…å¾—åˆ†: {best_score:.4f} (åŒ¹é…: {best_speaker}, å·®è·: {confidence_gap:.4f})"
                    )

                    # å¯¹äºè¾¹ç•Œæƒ…å†µï¼ˆåˆ†æ•°æ¥è¿‘é˜ˆå€¼ä¸”ç½®ä¿¡åº¦å·®è·å°ï¼‰ï¼Œä¼˜å…ˆä¿æŒè¿ç»­æ€§
                    if previous_speaker and best_speaker != previous_speaker:
                        prev_score = 0.0
                        if previous_speaker in self.speaker_registry:
                            prev_emb = self.speaker_registry[previous_speaker][
                                "embedding"
                            ]
                            prev_score = self._cosine_similarity(embedding, prev_emb)

                        # å¦‚æœä¸Šä¸€ä¸ªè¯´è¯äººçš„åˆ†æ•°ä¹Ÿè¶…è¿‡é˜ˆå€¼ä¸”å·®è·ä¸å¤§ï¼Œä¿æŒè¿ç»­æ€§
                        if prev_score > THRESHOLD and (best_score - prev_score) < 0.1:
                            best_speaker = previous_speaker
                            best_score = prev_score
                            logger.debug(f"ä¿æŒè¯´è¯äººè¿ç»­æ€§: {previous_speaker}")

                    if best_score > THRESHOLD:
                        # ä½¿ç”¨è‡ªé€‚åº”æƒé‡æ›´æ–°åµŒå…¥ - æ ·æœ¬è¶Šå¤šï¼Œæ–°æ ·æœ¬æƒé‡è¶Šå°
                        old_emb = self.speaker_registry[best_speaker]["embedding"]
                        count = self.speaker_registry[best_speaker]["count"]

                        # è‡ªé€‚åº”æƒé‡ï¼šcount=1æ—¶alpha=0.5, count=10æ—¶alphaâ‰ˆ0.9, count=50æ—¶alphaâ‰ˆ0.98
                        alpha = 1 - 1 / (1 + count * 0.5)
                        new_emb = alpha * old_emb + (1 - alpha) * embedding
                        # å½’ä¸€åŒ–
                        new_emb = new_emb / np.linalg.norm(new_emb)

                        self.speaker_registry[best_speaker]["embedding"] = new_emb
                        self.speaker_registry[best_speaker]["count"] = count + 1

                        gender = self.speaker_registry[best_speaker]["gender"]
                        return f"{best_speaker} ({gender})"
                    else:
                        # æ³¨å†Œæ–°è¯´è¯äºº
                        gender = self.detect_gender(audio_segment)

                        new_id = f"Speaker_{len(self.speaker_registry) + 1}"
                        self.speaker_registry[new_id] = {
                            "embedding": embedding,
                            "gender": gender,
                            "count": 1,
                        }
                        logger.info(
                            f"æ³¨å†Œæ–°è¯´è¯äºº: {new_id} ({gender}) (å¾—åˆ†: {best_score:.4f})"
                        )
                        return f"{new_id} ({gender})"

            logger.debug("æœªæ‰¾åˆ°è¯´è¯äººåµŒå…¥")
            return "æœªçŸ¥"
        except Exception as e:
            logger.error(f"è¯´è¯äººè¯†åˆ«é”™è¯¯: {e}")
            return "æœªçŸ¥"

    def check_audio_quality(
        self, audio_chunk: bytes, min_energy_threshold: float = 100.0
    ):
        """
        æ£€æŸ¥éŸ³é¢‘è´¨é‡,è¿‡æ»¤é™éŸ³æˆ–ä½èƒ½é‡ç‰‡æ®µã€‚

        Args:
            audio_chunk: éŸ³é¢‘å­—èŠ‚æ•°æ®
            min_energy_threshold: æœ€å°èƒ½é‡é˜ˆå€¼

        Returns:
            dict: {"is_valid": bool, "energy": float, "max_amplitude": int}
        """
        if len(audio_chunk) == 0:
            return {"is_valid": False, "energy": 0.0, "max_amplitude": 0}

        try:
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            audio_np = np.frombuffer(audio_chunk, dtype=np.int16)

            # è®¡ç®—æœ€å¤§æŒ¯å¹…
            max_amp = np.max(np.abs(audio_np))

            # è®¡ç®—èƒ½é‡ (RMS)
            audio_float = audio_np.astype(np.float32)
            energy = np.sqrt(np.mean(audio_float**2))

            # åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆè¯­éŸ³
            is_valid = energy >= min_energy_threshold and max_amp > 50

            return {
                "is_valid": is_valid,
                "energy": float(energy),
                "max_amplitude": int(max_amp),
            }

        except Exception as e:
            logger.error(f"éŸ³é¢‘è´¨é‡æ£€æµ‹é”™è¯¯: {e}")
            # å‡ºé”™æ—¶å‡è®¾éŸ³é¢‘æœ‰æ•ˆ,é¿å…ä¸¢å¤±æ•°æ®
            return {"is_valid": True, "energy": 0.0, "max_amplitude": 0}

    def detect_silence_segments(self, audio_chunk: bytes, window_ms: int = 50):
        """
        æ£€æµ‹éŸ³é¢‘ä¸­çš„é™éŸ³æ®µè½ï¼Œç”¨äºè¾…åŠ©VADè¿›è¡ŒäºŒæ¬¡åˆ†æ®µã€‚

        Args:
            audio_chunk: éŸ³é¢‘å­—èŠ‚æ•°æ®
            window_ms: æ»‘åŠ¨çª—å£å¤§å°ï¼ˆæ¯«ç§’ï¼‰

        Returns:
            list: é™éŸ³æ®µçš„èµ·æ­¢ä½ç½®åˆ—è¡¨ [(start_ms, end_ms), ...]
        """
        if len(audio_chunk) < self.BYTES_PER_MS * window_ms:
            return []

        try:
            audio_np = np.frombuffer(audio_chunk, dtype=np.int16).astype(np.float32)
            window_samples = int(self.SAMPLE_RATE * window_ms / 1000)
            hop_samples = window_samples // 2

            silence_segments = []
            silence_start = None

            for i in range(0, len(audio_np) - window_samples, hop_samples):
                window = audio_np[i : i + window_samples]
                energy = np.sqrt(np.mean(window**2))

                if energy < self.SILENCE_ENERGY_THRESHOLD:
                    if silence_start is None:
                        silence_start = i / self.SAMPLE_RATE * 1000
                else:
                    if silence_start is not None:
                        silence_end = i / self.SAMPLE_RATE * 1000
                        duration = silence_end - silence_start
                        if duration >= self.SILENCE_DURATION_MS:
                            silence_segments.append((silence_start, silence_end))
                        silence_start = None

            return silence_segments

        except Exception as e:
            logger.debug(f"é™éŸ³æ£€æµ‹é”™è¯¯: {e}")
            return []

    def _process_audio_segment(
        self,
        audio_bytes: bytes,
        session_id: str = "unknown",
        previous_speaker: str = None,
    ):
        """
        å¤„ç†å•ä¸ªéŸ³é¢‘ç‰‡æ®µï¼šé™å™ª -> è¯´è¯äººè¯†åˆ« -> ASR -> æ ‡ç‚¹

        Args:
            audio_bytes: éŸ³é¢‘å­—èŠ‚æ•°æ®
            session_id: ä¼šè¯ID
            previous_speaker: ä¸Šä¸€ä¸ªè¯´è¯äººIDï¼Œç”¨äºè¿ç»­æ€§åˆ¤æ–­
        """
        results = []

        # 1. æ£€æŸ¥é•¿åº¦
        if (
            len(audio_bytes)
            < self.SAMPLE_RATE * self.MIN_SEGMENT_LEN_SEC * self.BYTES_PER_SAMPLE
        ):
            return results

        # 2. è¯´è¯äººè¯†åˆ« (ä¼ é€’ä¸Šä¸€ä¸ªè¯´è¯äººä¿¡æ¯)
        speaker_id = self.recognize_speaker(audio_bytes, previous_speaker)

        # 3. å‡†å¤‡éŸ³é¢‘æ•°æ® (float32)
        audio_np = (
            np.frombuffer(audio_bytes, dtype=np.int16).astype(np.float32)
            / self.PCM_NORM_FACTOR
        )

        # 4. é™å™ª
        try:
            audio_np = nr.reduce_noise(
                y=audio_np, sr=self.SAMPLE_RATE, prop_decrease=self.NOISE_REDUCTION_PROP
            )
        except Exception as e:
            logger.error(f"Noise reduction failed: {e}")

        # 5. ASR è¯†åˆ«
        asr_text = ""
        try:
            asr_res = self.asr_model.generate(
                input=audio_np,
                cache={},  # å¥å­çº§åˆ«ä¸ä½¿ç”¨ç¼“å­˜
                is_final=True,
                batch_size=1,
                disable_pbar=True,
            )
            logger.debug(f"ASR Raw Result: {asr_res}")
            if isinstance(asr_res, list) and len(asr_res) > 0:
                asr_text = asr_res[0].get("text", "")
                import re

                asr_text = re.sub(r"<\|.*?\|>", "", asr_text).strip()
        except Exception as e:
            logger.error(f"ASR é”™è¯¯: {e}")

        # 6. æ ‡ç‚¹
        if asr_text and self.punc_model:
            try:
                punc_res = self.punc_model.generate(
                    input=asr_text, is_final=True, disable_pbar=True
                )
                if isinstance(punc_res, list) and len(punc_res) > 0:
                    asr_text = punc_res[0].get("text", asr_text)
            except Exception:
                pass

        if asr_text:
            logger.info(f"[{session_id[:8]}] è¯†åˆ«ç»“æœ: {speaker_id} - {asr_text}")
            results.append(
                {
                    "text": asr_text,
                    "speaker_id": speaker_id,
                    # timestamp éœ€è¦åœ¨å¤–éƒ¨æ·»åŠ 
                }
            )

        return results

    def start_worker(self):
        """Start the background worker for processing audio queue."""
        import asyncio

        if self.queue is None:
            self.queue = asyncio.Queue()

        if self.worker_task is None or self.worker_task.done():
            self.worker_task = asyncio.create_task(self._worker())
            logger.info("ASR background worker started.")

    async def stop_worker(self):
        """Stop the background worker gracefully."""
        if self.queue:
            await self.queue.join()  # Wait for all tasks to be done

        if self.worker_task:
            self.worker_task.cancel()
            try:
                await self.worker_task
            except asyncio.CancelledError:
                pass
            self.worker_task = None
            logger.info("ASR background worker stopped.")

    async def enqueue_audio(
        self, session_id: str, audio_chunk: bytes, is_final: bool = False
    ):
        """Add audio chunk to the processing queue."""
        if self.queue is None:
            self.start_worker()

        await self.queue.put((session_id, audio_chunk, is_final))

    def register_callback(self, session_id: str, callback):
        """Register a callback to receive results for a session."""
        self.callbacks[session_id] = callback

    def unregister_callback(self, session_id: str):
        """Unregister callback for a session."""
        if session_id in self.callbacks:
            del self.callbacks[session_id]

    async def _worker(self):
        """Background worker to process audio chunks from queue."""
        import asyncio

        logger.info("ASR Worker loop started")
        while True:
            try:
                session_id, audio_chunk, is_final = await self.queue.get()

                try:
                    # Process the chunk
                    # We run CPU-bound inference in a thread pool to avoid blocking the async loop
                    loop = asyncio.get_running_loop()
                    results = await loop.run_in_executor(
                        None,
                        lambda: self._process_stream(session_id, audio_chunk, is_final),
                    )

                    # Send results back via callback
                    if session_id in self.callbacks:
                        callback = self.callbacks[session_id]
                        if asyncio.iscoroutinefunction(callback):
                            await callback(results, is_final)
                        else:
                            callback(results, is_final)

                except Exception as e:
                    logger.error(
                        f"Error processing audio for session {session_id}: {e}"
                    )
                finally:
                    self.queue.task_done()

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Worker error: {e}")
                await asyncio.sleep(1)  # Prevent tight loop on error

    def _process_stream(
        self, session_id: str, audio_chunk: bytes, is_final: bool = False
    ):
        """
        å¤„ç†ç‰¹å®šä¼šè¯çš„éŸ³é¢‘æµã€‚
        ä½¿ç”¨ VAD è¿›è¡Œåˆ†æ®µï¼Œä»…åœ¨æ£€æµ‹åˆ°å®Œæ•´å¥å­ï¼ˆVAD ç‰‡æ®µç»“æŸï¼‰æ—¶è§¦å‘ ASR å’Œè¯´è¯äººè¯†åˆ«ã€‚
        """
        logger.debug(
            f"[{session_id[:8]}] inference_stream: {len(audio_chunk)} bytes, is_final={is_final}"
        )
        if self.asr_model is None:
            raise RuntimeError("Models not loaded. Call load_models() first.")

        # ä¸ºæ–°ä¼šè¯åˆå§‹åŒ–ç¼“å­˜
        if session_id not in self.cache:
            self.cache[session_id] = {
                "asr": {},
                "vad": {},
                "punc": {},
                "audio_buffer": bytearray(),  # ç´¯ç§¯éŸ³é¢‘ç¼“å†²åŒº
                "buffer_offset_bytes": 0,  # ç¼“å†²åŒºèµ·å§‹å­—èŠ‚ç›¸å¯¹äºä¼šè¯å¼€å§‹çš„åç§»é‡
                "vad_state": {"current_start_ms": -1, "segments": []},  # è·Ÿè¸ª VAD çŠ¶æ€
                "last_speaker": None,  # è¿½è¸ªä¸Šä¸€ä¸ªè¯´è¯äººç”¨äºè¿ç»­æ€§åˆ¤æ–­
            }

        session_cache = self.cache[session_id]

        # 1. è¿½åŠ åˆ°ç¼“å†²åŒº
        session_cache["audio_buffer"].extend(audio_chunk)

        # è½¬æ¢å½“å‰ chunk ä¸º float32 ç”¨äº VAD
        if len(audio_chunk) > 0:
            audio_np = (
                np.frombuffer(audio_chunk, dtype=np.int16).astype(np.float32)
                / self.PCM_NORM_FACTOR
            )
        else:
            audio_np = np.array([], dtype=np.float32)

        # 2. è¿è¡Œ VAD (è¿ç»­)
        vad_segments = []
        if len(audio_np) > 0:
            try:
                vad_res = self.vad_model.generate(
                    input=audio_np,
                    cache=session_cache["vad"],
                    is_final=is_final,
                    batch_size=1,
                    chunk_size=self.VAD_CHUNK_SIZE,  # VAD å—å¤§å°
                    disable_pbar=True,
                )
                # VAD è¿”å›çš„æ˜¯ç›¸å¯¹äºæœ¬æ¬¡è¾“å…¥æµçš„ç»å¯¹æ—¶é—´æˆ³
                if isinstance(vad_res, list) and len(vad_res) > 0:
                    vad_segments = vad_res[0].get("value", [])
            except Exception as e:
                logger.error(f"[{session_id[:8]}] VAD é”™è¯¯: {e}")

            if len(vad_segments) > 0:
                logger.debug(f"[{session_id[:8]}] VAD Segments: {vad_segments}")

        results = []

        # 3. å¤„ç† VAD ç‰‡æ®µ
        # æˆ‘ä»¬éœ€è¦ç»´æŠ¤ä¸€ä¸ªå…¨å±€çš„ VAD çŠ¶æ€ï¼Œå› ä¸º segments å¯èƒ½è·¨è¶Š chunk

        for seg in vad_segments:
            start_ms, end_ms = seg

            # VAD è¾“å‡º -1 è¡¨ç¤ºæœªå¼€å§‹æˆ–æœªç»“æŸ
            if start_ms != -1:
                session_cache["vad_state"]["current_start_ms"] = start_ms
                logger.debug(f"[{session_id[:8]}] Speech started at {start_ms}ms (VAD)")

            if end_ms != -1:
                # å¥å­ç»“æŸ
                start_ms_stored = session_cache["vad_state"]["current_start_ms"]

                if start_ms_stored != -1:
                    # è®¡ç®— byte åç§» (16kHz, 16bit = 32 bytes/ms)
                    # ä½¿ç”¨ç»å¯¹å­—èŠ‚åç§»è®¡ç®—ï¼Œé¿å…ç´¯ç§¯è¯¯å·®

                    abs_start_byte = int(start_ms_stored * self.BYTES_PER_MS)
                    abs_end_byte = int(end_ms * self.BYTES_PER_MS)

                    buffer_offset_bytes = session_cache["buffer_offset_bytes"]

                    start_byte = abs_start_byte - buffer_offset_bytes
                    end_byte = abs_end_byte - buffer_offset_bytes

                    if start_byte < 0:
                        # è¯´æ˜å¼€å§‹ç‚¹å·²ç»è¢«ç§»å‡º buffer äº†
                        logger.warning(
                            f"[{session_id[:8]}] ç‰‡æ®µå¼€å§‹ç‚¹å·²ä¸¢å¤± (å»¶è¿Ÿ: {-start_byte} bytes)"
                        )
                        start_byte = 0

                    # å®¹é”™å¤„ç†ï¼šå¦‚æœ end_byte ç¨å¾®è¶…å‡º buffer (ä¾‹å¦‚ < 100ms / 3200 bytes)ï¼Œ
                    # å¯èƒ½æ˜¯ VAD æ—¶é—´æˆ³çš„èˆå…¥è¯¯å·®æˆ–å¾®å°çš„æ—¶åºä¸åŒ¹é…ã€‚
                    # åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æˆªæ–­åˆ° buffer ç»“å°¾å¹¶å¤„ç†ï¼Œè€Œä¸æ˜¯ç­‰å¾…æ°¸è¿œä¸ä¼šåˆ°æ¥çš„æ•°æ®ã€‚
                    buffer_len = len(session_cache["audio_buffer"])
                    if end_byte > buffer_len and end_byte - buffer_len < 3200:
                        logger.debug(
                            f"[{session_id[:8]}] VAD ç»“æŸç‚¹å¾®è°ƒ: {end_byte} -> {buffer_len}"
                        )
                        end_byte = buffer_len

                    if end_byte <= buffer_len:
                        segment_audio = session_cache["audio_buffer"][
                            start_byte:end_byte
                        ]

                        # 4. å¯¹è¯¥ç‰‡æ®µè¿›è¡Œ ASR å’Œ è¯´è¯äººè¯†åˆ«
                        # ä½¿ç”¨ helper methodï¼Œä¼ é€’ä¸Šä¸€ä¸ªè¯´è¯äººä¿¡æ¯
                        previous_speaker = session_cache.get("last_speaker")
                        seg_results = self._process_audio_segment(
                            segment_audio, session_id, previous_speaker
                        )

                        for res in seg_results:
                            res["timestamp"] = end_ms / 1000.0
                            res["vad_segment"] = [start_ms, end_ms]
                            results.append(res)
                            # æ›´æ–° last_speaker
                            if res.get("speaker_id") and res["speaker_id"] != "æœªçŸ¥":
                                session_cache["last_speaker"] = res["speaker_id"]

                        # 5. æ¸…ç† Buffer
                        # æˆ‘ä»¬å¯ä»¥å®‰å…¨åœ°ç§»é™¤ end_byte ä¹‹å‰çš„æ•°æ®
                        # æ›´æ–° offset
                        del session_cache["audio_buffer"][:end_byte]
                        session_cache["buffer_offset_bytes"] += end_byte

                    else:
                        # æ•°æ®è¿˜ä¸å¤Ÿï¼Œç­‰å¾…æ›´å¤šæ•°æ®
                        pass

                    # é‡ç½®å¼€å§‹æ—¶é—´
                    session_cache["vad_state"]["current_start_ms"] = -1

        # 3.5 æ£€æŸ¥é•¿è¯­éŸ³å¼ºåˆ¶æ–­å¥ / VAD å¤±æ•ˆå…œåº•
        # å¦‚æœå½“å‰ buffer ç§¯å‹è¶…è¿‡ 2 ç§’ä¸”æ²¡æœ‰æ£€æµ‹åˆ°å¼€å§‹ï¼Œå¼ºåˆ¶è®¤ä¸ºå¼€å§‹
        # å¦‚æœå·²ç»å¼€å§‹ä¸”è¶…è¿‡ 10 ç§’ï¼Œå¼ºåˆ¶ç»“æŸ

        current_start_ms = session_cache["vad_state"]["current_start_ms"]
        buffer_len_bytes = len(session_cache["audio_buffer"])

        # å…œåº•é€»è¾‘ï¼šå¦‚æœ buffer å¤ªé•¿ (> 5s) ä¸”æ²¡æœ‰ start_msï¼Œå¼ºåˆ¶è®¾ç½® start
        if (
            current_start_ms == -1
            and buffer_len_bytes > self.SAMPLE_RATE * self.BYTES_PER_SAMPLE * 5
        ):  # 5 seconds
            # æˆ‘ä»¬å‡è®¾è¯­éŸ³ä» buffer å¼€å¤´å°±å¼€å§‹äº†
            # è®¡ç®— buffer å¼€å¤´å¯¹åº”çš„æ—¶é—´æˆ³
            buffer_start_ms = session_cache["buffer_offset_bytes"] / float(
                self.BYTES_PER_MS
            )
            session_cache["vad_state"]["current_start_ms"] = buffer_start_ms
            current_start_ms = buffer_start_ms
            logger.info(
                f"[{session_id[:8]}] Buffer ç§¯å‹ ({buffer_len_bytes} bytes)ï¼Œå¼ºåˆ¶è§¦å‘è¯­éŸ³å¼€å§‹: {buffer_start_ms:.0f}ms"
            )

        if current_start_ms != -1:
            # è®¡ç®—å½“å‰ buffer ç»“å°¾å¯¹åº”çš„æ—¶é—´æˆ³
            # buffer_offset_bytes æ˜¯ buffer[0] çš„ç»å¯¹åç§»
            # len(buffer) æ˜¯ buffer é•¿åº¦
            # current_audio_end_byte = session_cache["buffer_offset_bytes"] + len(session_cache["audio_buffer"])
            # current_audio_end_ms = current_audio_end_byte // 32

            # ç®€åŒ–è®¡ç®—ï¼šæŒç»­æ—¶é—´ = (å½“å‰ buffer é•¿åº¦ + (buffer_offset - start_byte)) / 32
            # start_byte = start_ms * 32

            abs_start_byte = int(current_start_ms * self.BYTES_PER_MS)
            current_abs_end_byte = session_cache["buffer_offset_bytes"] + len(
                session_cache["audio_buffer"]
            )

            duration_bytes = current_abs_end_byte - abs_start_byte
            duration_ms = duration_bytes / float(self.BYTES_PER_MS)

            MAX_DURATION_MS = self.MAX_SEGMENT_DURATION_MS

            # --- Partial Result Logic ---
            # Check if we should generate a partial result
            last_partial_time = session_cache.get("last_partial_time", 0)
            now = time.time()

            if (
                now - last_partial_time > 0.5 and duration_ms > 500
            ):  # Every 500ms, if segment > 500ms
                session_cache["last_partial_time"] = now

                # Extract current segment
                start_byte = abs_start_byte - session_cache["buffer_offset_bytes"]
                if start_byte < 0:
                    start_byte = 0

                partial_audio = session_cache["audio_buffer"][start_byte:]

                if (
                    len(partial_audio)
                    > self.SAMPLE_RATE
                    * self.MIN_SEGMENT_LEN_SEC
                    * self.BYTES_PER_SAMPLE
                ):
                    seg_audio_np = (
                        np.frombuffer(partial_audio, dtype=np.int16).astype(np.float32)
                        / self.PCM_NORM_FACTOR
                    )
                    try:
                        # Use is_final=False for partials if supported, or True but mark result as partial
                        # SenseVoice/Paraformer usually treat input as a sentence.
                        asr_res = self.asr_model.generate(
                            input=seg_audio_np,
                            cache={},
                            is_final=False,  # Partial
                            batch_size=1,
                            disable_pbar=True,
                        )
                        if isinstance(asr_res, list) and len(asr_res) > 0:
                            partial_text = asr_res[0].get("text", "")
                            import re

                            partial_text = re.sub(
                                r"<\|.*?\|>", "", partial_text
                            ).strip()

                            if partial_text:
                                results.append(
                                    {
                                        "text": partial_text,
                                        "speaker_id": "Partial",
                                        "timestamp": time.time(),
                                        "vad_segment": [],
                                        "is_partial": True,
                                    }
                                )
                    except Exception as e:
                        logger.debug(f"Partial ASR error: {e}")

            # --- End Partial Logic ---

            if duration_ms > MAX_DURATION_MS:
                logger.info(
                    f"[{session_id[:8]}] æ£€æµ‹åˆ°é•¿è¯­éŸ³ ({duration_ms:.0f}ms)ï¼Œå¼ºåˆ¶æ–­å¥"
                )

                # è®©æˆ‘ä»¬æˆªæ–­åˆ°å½“å‰ buffer ç»“å°¾ï¼Œä½œä¸ºä¸€æ®µ
                force_end_ms = current_start_ms + duration_ms

                # è®¡ç®—ç›¸å¯¹åç§»
                start_byte = abs_start_byte - session_cache["buffer_offset_bytes"]
                end_byte = len(session_cache["audio_buffer"])  # å…¨éƒ¨ç”¨å®Œ

                if start_byte < 0:
                    start_byte = 0

                if end_byte > start_byte:
                    segment_audio = session_cache["audio_buffer"][start_byte:end_byte]

                    # ä½¿ç”¨ helper methodï¼Œä¼ é€’ä¸Šä¸€ä¸ªè¯´è¯äººä¿¡æ¯
                    previous_speaker = session_cache.get("last_speaker")
                    seg_results = self._process_audio_segment(
                        segment_audio, session_id, previous_speaker
                    )

                    for res in seg_results:
                        res["timestamp"] = force_end_ms / 1000.0
                        res["vad_segment"] = [current_start_ms, force_end_ms]
                        res["is_partial"] = False
                        results.append(res)
                        # æ›´æ–° last_speaker
                        if res.get("speaker_id") and res["speaker_id"] != "æœªçŸ¥":
                            session_cache["last_speaker"] = res["speaker_id"]

                    # æ¸…ç† buffer
                    del session_cache["audio_buffer"][:end_byte]
                    session_cache["buffer_offset_bytes"] += end_byte

                    # æ›´æ–° start_ms ä¸ºå½“å‰ç»“æŸæ—¶é—´ï¼Œç›¸å½“äºå¼€å§‹æ–°çš„ä¸€æ®µ
                    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å®é™…ä¸ŠæŠŠè¿ç»­çš„è¯­éŸ³åˆ‡æ–­äº†ã€‚
                    # ä¸‹ä¸€æ®µçš„å¼€å§‹æ—¶é—´åº”è¯¥æ˜¯ force_end_ms
                    session_cache["vad_state"]["current_start_ms"] = force_end_ms

        # 6. å¤„ç† is_final
        if is_final and len(session_cache["audio_buffer"]) > 0:
            # å¤„ç†å‰©ä½™çš„æ‰€æœ‰éŸ³é¢‘
            remaining_audio = session_cache["audio_buffer"]

            previous_speaker = session_cache.get("last_speaker")
            seg_results = self._process_audio_segment(
                remaining_audio, session_id, previous_speaker
            )
            for res in seg_results:
                res["timestamp"] = time.time()
                res["vad_segment"] = []
                res["is_partial"] = False
                results.append(res)

            # æ¸…ç†
            del self.cache[session_id]

        return results

    def transcribe_file(self, file_path: str):
        """
        å¯¹éŸ³é¢‘æ–‡ä»¶è¿›è¡Œå®Œæ•´è½¬å†™ (VAD -> Speaker -> ASR -> Punc)
        """
        if self.asr_model is None:
            self.load_models()

        if not os.path.exists(file_path):
            raise FileNotFoundError(f"File not found: {file_path}")

        import librosa

        # Load audio (resample to 16000)
        try:
            audio, _ = librosa.load(file_path, sr=self.SAMPLE_RATE, mono=True)
            # Convert to float32 (librosa loads as float32, normalized -1 to 1)
            # FunASR expects float32 or int16.
            # Our previous logic used int16 bytes -> float32.
            # Here we have float32 directly.
        except Exception as e:
            logger.error(f"Failed to load audio file: {e}")
            raise e

        results = []

        # 1. VAD
        vad_segments = []
        try:
            # FunASR VAD can handle long audio
            vad_res = self.vad_model.generate(
                input=audio, batch_size=1, disable_pbar=True
            )
            if isinstance(vad_res, list) and len(vad_res) > 0:
                vad_segments = vad_res[0].get("value", [])
        except Exception as e:
            logger.error(f"VAD failed for file {file_path}: {e}")
            # Fallback: treat whole file as one segment if short, or fail?
            # Let's try to proceed with whole file if VAD fails
            vad_segments = [[0, len(audio) / self.SAMPLE_RATE * 1000]]

        logger.info(f"File VAD segments: {len(vad_segments)}")

        # 2. Process segments
        for seg in vad_segments:
            start_ms, end_ms = seg
            if start_ms == -1 or end_ms == -1:
                continue

            start_sample = int(start_ms * self.SAMPLES_PER_MS)  # ms * 16 samples/ms

            end_sample = int(end_ms * self.SAMPLES_PER_MS)

            segment_audio = audio[start_sample:end_sample]

            if (
                len(segment_audio) < self.SAMPLE_RATE * self.MIN_SEGMENT_LEN_SEC
            ):  # Skip < 0.2s
                continue

            # OR just convert back to bytes here.
            segment_int16 = (segment_audio * self.PCM_NORM_FACTOR).astype(np.int16)
            segment_bytes = segment_int16.tobytes()

            speaker_id = self.recognize_speaker(segment_bytes)

            # Apply Noise Reduction
            try:
                segment_audio = nr.reduce_noise(
                    y=segment_audio,
                    sr=self.SAMPLE_RATE,
                    prop_decrease=self.NOISE_REDUCTION_PROP,
                )
            except Exception as e:
                logger.error(f"Noise reduction failed: {e}")

            asr_text = ""
            try:
                asr_res = self.asr_model.generate(
                    input=segment_audio,
                    cache={},
                    is_final=True,
                    batch_size=1,
                    disable_pbar=True,
                )
                if isinstance(asr_res, list) and len(asr_res) > 0:
                    asr_text = asr_res[0].get("text", "")
                    import re

                    asr_text = re.sub(r"<\|.*?\|>", "", asr_text).strip()
            except Exception as e:
                logger.error(f"ASR error: {e}")

            # C. Punctuation
            if asr_text and self.punc_model:
                try:
                    punc_res = self.punc_model.generate(
                        input=asr_text, is_final=True, disable_pbar=True
                    )
                    if isinstance(punc_res, list) and len(punc_res) > 0:
                        asr_text = punc_res[0].get("text", asr_text)
                except Exception:
                    pass

            if asr_text:
                results.append(
                    {
                        "text": asr_text,
                        "speaker": speaker_id,  # Note: using 'speaker' to match TranscriptItem schema
                        "timestamp": start_ms / 1000.0,  # Use start time for transcript
                    }
                )

        return results

    def reset_session(self, session_id: str):
        """æ¸…é™¤ä¼šè¯ç¼“å­˜ã€‚"""
        if session_id in self.cache:
            del self.cache[session_id]


asr_engine = ASREngine()
